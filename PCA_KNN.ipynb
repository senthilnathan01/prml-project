{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and k-Nearest Neighbours\n",
    "\n",
    "Consider the Digits dataset that is a part of the sklearn library. It consists of 1797 64 dimensional vectors with each corresponding to an 8x8 image of a digit. The label also gives the digit id. It is a 10-class classification problem.\n",
    "\n",
    "Choose a random subset of size 1500 for train and the rest for testing. Run k-Nearest neighbours with k values 1,3,7,15 and 31 and report the training and test accuracy. \n",
    "\n",
    "Repeat the above after performing PCA on the data. Use top n-principal components for n=2,4,8,16,32. For each n in the list report the best k-NN test accuracy and the k which achieves that accuracy and the approximation error for this particular value of n.\n",
    "\n",
    "Repeat the above for a noisy version of the data. i.e. add a random Gaussian noise of mean zero and variance 1 to all the 1797*64 input numbers.\n",
    "\n",
    "In total, the results should be given in 4 tables in the last textwrite cell:. Summarise your findings in a paragraph.\n",
    "\n",
    "Table 1: Raw data , k-NN performance. One row for each k.\n",
    "\n",
    "Table 2: n-component PCA preprocessed data k-NN performance. One row for each n.\n",
    "\n",
    "Table 3: Raw noised data, k-NN performance. One row for each k.\n",
    "\n",
    "Table 4: n-component PCA preprocessed noised data k-NN performance. One row for each n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codewrite cell (Use as you wish)\n",
    "\n",
    "# Train-test split function\n",
    "def train_test_split(X, y, size_train, random_state=None):\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    size_train = int(size_train)\n",
    "    train_indices = indices[:size_train]\n",
    "    test_indices = indices[size_train:]\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "# PCA implementation\n",
    "def pca(X, n_components):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    X_ = X - mean\n",
    "    cov_mat = np.cov(X_, rowvar=False) \n",
    "    # rowvar=False means that each column represents a feature, and each row represents an observation data point\n",
    "    eigen_values, eigen_vectors = np.linalg.eigh(cov_mat)\n",
    "    sorted_index = np.argsort(eigen_values)[::-1]\n",
    "    sorted_eigenvectors = eigen_vectors[:,sorted_index]\n",
    "    sorted_eigenvectors = sorted_eigenvectors[:, :n_components]\n",
    "    X_reduced = np.dot(X_, sorted_eigenvectors)\n",
    "    return X_reduced, sorted_eigenvectors, mean\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "# KNN implementation\n",
    "def k_nearest_neighbors(X_train, y_train, X_test, k):\n",
    "    y_pred = []\n",
    "    for test_point in X_test:\n",
    "        distances = []\n",
    "        for i, train_point in enumerate(X_train):\n",
    "            distance = euclidean_distance(test_point, train_point)\n",
    "            distances.append((distance, y_train[i]))\n",
    "        distances.sort(key=lambda x: x[0])\n",
    "        k_nearest_labels = [label for _, label in distances[:k]]\n",
    "        majority_vote = np.argmax(np.bincount(k_nearest_labels))\n",
    "        y_pred.append(majority_vote)\n",
    "    return np.array(y_pred)\n",
    "\n",
    "# Accuracy calculation\n",
    "def accuracy(y_true, y_pred):\n",
    "    return (np.sum(y_true == y_pred) / len(y_true))*100\n",
    "\n",
    "def run_knn(X_train, X_test, y_train, y_test, k_values):\n",
    "    results = []\n",
    "    for k in k_values:\n",
    "        y_train_pred = k_nearest_neighbors(X_train, y_train, X_train, k)\n",
    "        train_acc = accuracy(y_train, y_train_pred)\n",
    "        y_test_pred = k_nearest_neighbors(X_train, y_train, X_test, k)\n",
    "        test_acc = accuracy(y_test, y_test_pred)\n",
    "        results.append((k, train_acc, test_acc))\n",
    "    return results\n",
    "\n",
    "# Loading the dataset\n",
    "X, y = load_digits().data, load_digits().target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, size_train=1500, random_state=42)\n",
    "\n",
    "# Define k values\n",
    "k_values = [1, 3, 7, 15, 31]\n",
    "\n",
    "# PCA n values\n",
    "pca_n_values = [2, 4, 8, 16, 32]\n",
    "\n",
    "# Print result tables\n",
    "def print_table(header, data):\n",
    "    print(header)\n",
    "    for row in data:\n",
    "        print(\"  \".join(f\"{item:.4f}\" if isinstance(item, float) else str(item) for item in row))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the experiments for filling Tables 1 and 2 here\n",
    "# Raw data k-NN performance\n",
    "raw_results = run_knn(X_train, X_test, y_train, y_test, k_values)\n",
    "\n",
    "# print(\"Table 1: Raw data k-NN performance\")\n",
    "# print_table([\"k\", \"Train Accuracy\", \"Test Accuracy\"], raw_results)\n",
    "\n",
    "pca_results = []\n",
    "\n",
    "for n in pca_n_values:\n",
    "    X_train_pca, eigenvectors, mean= pca(X_train, n)\n",
    "    X_test_pca = np.dot(X_test - mean, eigenvectors)\n",
    "    pca_knn_results = run_knn(X_train_pca, X_test_pca, y_train, y_test, k_values)\n",
    "    best_k, best_train_acc, best_test_acc = max(pca_knn_results, key=lambda item: item[2])\n",
    "    approximation_error = np.mean((X_test - np.dot(X_test_pca, eigenvectors.T) - mean) ** 2)\n",
    "    pca_results.append((n, best_k, best_test_acc, approximation_error))\n",
    "\n",
    "# print(\"Table 2: PCA-preprocessed data k-NN performance\")\n",
    "# print_table([\"n\", \"Best k\", \"Best Test Accuracy\", \"Approximation Error\"], pca_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the experiments for filling Tables 3 and 4 here\n",
    "\n",
    "# Adding Gaussian noise to data\n",
    "np.random.seed(42)\n",
    "X_noised = X + np.random.normal(0, 1, X.shape)\n",
    "X_train_noised, X_test_noised, y_train, y_test = train_test_split(X_noised, y, size_train=1500, random_state=42)\n",
    "\n",
    "# Noised raw data k-NN performance\n",
    "noised_raw_results = run_knn(X_train_noised, X_test_noised, y_train, y_test, k_values)\n",
    "\n",
    "# print(\"Table 3: Noised raw data k-NN performance\")\n",
    "# print_table([\"k\", \"Train Accuracy\", \"Test Accuracy\"], noised_raw_results)\n",
    "\n",
    "noised_pca_results = []\n",
    "\n",
    "for n in pca_n_values:\n",
    "    X_train_noised_pca, eigenvectors, mean = pca(X_train_noised, n)\n",
    "    X_test_noised_pca = np.dot(X_test_noised - mean, eigenvectors)\n",
    "    noised_pca_knn_results = run_knn(X_train_noised_pca, X_test_noised_pca, y_train, y_test, k_values)\n",
    "    best_k, best_train_acc, best_test_acc = max(noised_pca_knn_results, key=lambda item: item[2])\n",
    "    approximation_error = np.mean((X_test_noised - np.dot(X_test_noised_pca, eigenvectors.T) - mean) ** 2)\n",
    "    noised_pca_results.append((n, best_k, best_test_acc, approximation_error))\n",
    "\n",
    "# print(\"Table 4: PCA-preprocessed noised data k-NN performance\")\n",
    "# print_table([\"n\", \"Best k\", \"Best Test Accuracy\", \"Approximation_error\"], noised_pca_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textwrite cell\n",
    "\n",
    "**Table 1: Raw data k-NN performance**\n",
    "\n",
    "k | Train Accuracy | Test Accuracy \n",
    "--- | --- | --- \n",
    "1 | 100.00% |  98.32% \n",
    "3 |  99.27% |   98.65% \n",
    "7 | 98.87%  |  97.98% \n",
    "15 | 98.33% |  97.31% \n",
    "31 | 97.00% |  95.96% \n",
    "\n",
    "**Table 2: PCA-preprocessed data k-NN performance**\n",
    "\n",
    "n | Best k | Best Test Accuracy | Approxiamation error\n",
    "--- | --- | --- | ---\n",
    "2 | 15 |  64.65% | 13.43\n",
    "4 |  7 |   87.54% | 9.64\n",
    "8 | 3  |  96.30% | 6.22\n",
    "16 | 3 |  98.99% | 2.84\n",
    "32 | 3 |  98.65% | 0.65\n",
    "\n",
    "**Table 3: Noised raw data k-NN performance**\n",
    "\n",
    "k | Train Accuracy | Test Accuracy \n",
    "--- | --- | --- \n",
    "1 | 100.00% |  98.32% \n",
    "3 |  99.20% |   98.65% \n",
    "7 | 98.73%  |  98.32% \n",
    "15 | 98.26% |  97.64% \n",
    "31 | 97.13% |  95.96% \n",
    "\n",
    "**Table 4: PCA-preprocessed data k-NN performance**\n",
    "\n",
    "n | Best k | Best Test Accuracy | Approxiamation error\n",
    "--- | --- | --- | ---\n",
    "2 | 15 |  65.32% | 14.34\n",
    "4 |  15 |   86.19% | 10.52\n",
    "8 | 3  |  96.30% | 7.05\n",
    "16 | 3 |  98.32% | 3.55\n",
    "32 | 1 |  98.32% | 1.18\n",
    "\n",
    "**Summary of Findings**\n",
    "\n",
    "#### 1. Raw Data k-NN Performance\n",
    "- The results show that the k-NN algorithm performs well on the raw data.\n",
    "- The training accuracy decreases slightly as k increases.\n",
    "- The test accuracy is consistently high across different k values, with the highest accuracy at k=3 (98.65%).\n",
    "\n",
    "#### 2. PCA-Preprocessed Data k-NN Performance\n",
    "- The performance improves significantly as the number of principal components increases.\n",
    "- For small numbers of components (e.g., n=2), the accuracy is much lower (64.65%), indicating that the data is not well-represented.\n",
    "- The accuracy improves as more components are used, reaching near raw data performance at n=16 and n=32.\n",
    "\n",
    "#### 3. Noised Raw Data k-NN Performance\n",
    "- Test accuracy is consistent across different k values, with the highest accuracy again at k=3 (98.65%).\n",
    "\n",
    "#### 4. PCA-Preprocessed Noised Data k-NN Performance\n",
    "- The accuracy for the noised PCA-preprocessed data follows a similar trend to the clean PCA data.\n",
    "- For small numbers of components (e.g., n=2), the accuracy is low (65.32%).\n",
    "- As the number of components increases, the accuracy improves, nearing the performance of the noised raw data at n=16 and n=32.\n",
    "\n",
    "### Explanation for Observed Values\n",
    "1. **Raw Data Performance**: The high performance on raw data is likely due to the high-dimensional feature space that k-NN can effectively utilize.\n",
    "\n",
    "2. **PCA-Preprocessed Data Performance**:\n",
    "    - With fewer components, significant information loss occurs, leading to lower accuracy.\n",
    "    - As the number of components increases, more information is retained, improving accuracy.\n",
    "    - With n=16 and n=32 components, most of the variance in the data is captured, resulting in accuracy close to the raw data.\n",
    "\n",
    "3. **Effect of Noise**:\n",
    "    - The noise introduces some distortion, but k-NN is robust to a degree of noise.\n",
    "    - The performance on noised raw data remains high because the noise level is not enough to significantly disrupt the high-dimensional structure that k-NN relies on.\n",
    "    - PCA helps mitigate noise by reducing dimensionality, which can help focus on the most relevant features, but the initial reduction in dimensions must be carefully chosen to avoid information loss.\n",
    "\n",
    "4. **Approximation Error**:\n",
    "    - The error is calculated as the difference between the test accuracy on raw data and PCA-preprocessed data.\n",
    "    - Higher approximation error at lower n values indicates significant information loss.\n",
    "    - Lower approximation error at higher n values indicates that PCA is capturing more of the data's variance, preserving performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
