{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "\n",
    "Write code for learning using Lasso Regression and give your conclusions. Use the dataset LassoReg_data.npz for this question. The file contains two matrices of size 120\\*1000 and 120\\*1, corresponding to 120 instance points with 1000 dimensional features and its targets.\n",
    "\n",
    " Split the data into train-validation-test on 50-25-25 ratio. Learn the best model using Lasso Regression (use projected gradient descent, the projection oracle code is given for your convenience). Try different learning rate parameters and L1 norm ball constraint radii. Choose an appropriate learning rate that allows for convergence of the training loss.  Train the models for different L1 norm radius parameters. Choose the L1 norm constraint that works best on the validation set. \n",
    "\n",
    "Report the test error of the learned model thus chosen. Also report the indices and weight values corresponding to the top 10 values of the weight vector (which is 1000 dimensional). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_oracle_l1(w, l1_norm):\n",
    "    # first remember signs and store them. Modify w so that it is all positive then.\n",
    "    signs = np.sign(w)\n",
    "    w = w*signs\n",
    "    # project this modified w onto the simplex in first orthant.\n",
    "    d=len(w)\n",
    "    # if w is already in l1 norm ball return as it is.\n",
    "    if np.sum(w)<=l1_norm:\n",
    "        return w*signs\n",
    "    \n",
    "    # using 1e-7 as zero here to avoid floating point issues\n",
    "    for i in range(d):\n",
    "        w_next = w+0\n",
    "        w_next[w>1e-7] = w[w>1e-7] - np.min(w[w>1e-7])\n",
    "        if np.sum(w_next)<=l1_norm:\n",
    "            w = ((l1_norm - np.sum(w_next))*w + (np.sum(w) - l1_norm)*w_next)/(np.sum(w)-np.sum(w_next))\n",
    "            return w*signs\n",
    "        else:\n",
    "            w=w_next\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"Data/LassoReg_data.npz\")\n",
    "X, Y= data[\"arr_0\"], data[\"arr_1\"]\n",
    "\n",
    "X_train = X[:60, :]\n",
    "Y_train = Y[:60]\n",
    "\n",
    "X_val = X[60:90, :]\n",
    "Y_val = Y[60:90]\n",
    "\n",
    "X_test = X[90:120, :]\n",
    "Y_test = Y[90:120]\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def lasso_regression(X_train, Y_train, X_val, Y_val, learning_rate, l1_norm, num_iter=100, tolerance=1e-4):\n",
    "    # Initialize weights\n",
    "    w = np.zeros(X_train.shape[1])\n",
    "    for _ in range(num_iter):\n",
    "        # Compute gradient\n",
    "        grad = -2 * X_train.T.dot(Y_train - X_train.dot(w))\n",
    "        # Update weights using projected gradient descent\n",
    "        w_new = w - learning_rate * grad\n",
    "        w = projection_oracle_l1(w_new, l1_norm)\n",
    "        # Evaluate validation error\n",
    "        val_loss = mean_squared_error(Y_val, X_val.dot(w))\n",
    "        # Check convergence\n",
    "        if np.linalg.norm(w - w_new) < tolerance:\n",
    "            break\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 0.001\n",
      "Best l1 norm: 1\n",
      "Test Mean Squared Error: 0.10451966512105049\n",
      "Top 10 indices: [339 340 341 342 343 332  81 686 390 107]\n",
      "Corresponding weight values: [ 0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  3.20391903e-08  9.87846601e-08\n",
      "  1.38372616e-01  8.61626690e-01]\n"
     ]
    }
   ],
   "source": [
    "# Write the code for the gradient descent routine on the training set mean square error loss function.\n",
    "# Also write code for doing validation of the learned model using the validation set\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rates = [0.001, 0.01, 0.01, 0.1]\n",
    "l1_norms = [1, 5, 10]\n",
    "\n",
    "# Grid search for best hyperparameters\n",
    "best_loss = float('inf')\n",
    "best_lr = None\n",
    "best_l1_norm = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for l1_norm in l1_norms:\n",
    "        # Train model\n",
    "        weights = lasso_regression(X_train, Y_train, X_val, Y_val, learning_rate=lr, l1_norm=l1_norm)\n",
    "        # Evaluate validation error\n",
    "        val_loss = mean_squared_error(Y_val, X_val.dot(weights))\n",
    "        # Update best hyperparameters if necessary\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_lr = lr\n",
    "            best_l1_norm = l1_norm\n",
    "print(f\"Best learning rate: {best_lr}\")\n",
    "print(f\"Best l1 norm: {best_l1_norm}\")\n",
    "best_weights = lasso_regression(X_train, Y_train, X_val, Y_val, learning_rate=best_lr, l1_norm=best_l1_norm)\n",
    "test_loss = mean_squared_error(Y_test, X_test.dot(best_weights))\n",
    "\n",
    "# Report test error\n",
    "print(\"Test Mean Squared Error:\", test_loss)\n",
    "\n",
    "# Report top 10 indices and weight values\n",
    "top_indices = np.argsort(best_weights)[-10:]\n",
    "top_weights = best_weights[top_indices]\n",
    "\n",
    "print(\"Top 10 indices:\", top_indices)\n",
    "print(\"Corresponding weight values:\", top_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "Best learning rate: 0.001 (For number of iterations = 100)\n",
    "\n",
    "Best l1 norm: 1\n",
    "\n",
    "Test Mean Squared Error: 0.10451966512105049\n",
    "\n",
    "Top 10 indices: [339 340 341 342 343 332  81 686 390 107]\n",
    "\n",
    "Corresponding weight values: [ 0.00000000e+00    -0.00000000e+00     0.00000000e+00 -0.00000000e+00   0.00000000e+00     -0.00000000e+00     3.20391903e-08             9.87846601e-08    1.38372616e-01     8.61626690e-01]\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
